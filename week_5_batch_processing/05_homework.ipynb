{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/28 19:59:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/28 19:59:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/28 19:59:26 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1001 fhv_tripdata_2019-10.csv > head_hw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pandas = pd.read_csv('head_hw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', DoubleType(), True), StructField('DOlocationID', DoubleType(), True), StructField('SR_Flag', DoubleType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PUlocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOlocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "\t.option(\"header\", \"true\") \\\n",
    "\t.schema(schema) \\\n",
    "\t.csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(6).write.parquet('data/pq/fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/pq/fhv/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhv_trip_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B01537|2019-10-02 07:09:00|2019-10-02 07:23:00|         264|         264|   NULL|                B01537|\n",
      "|              B00860|2019-10-02 01:58:19|2019-10-02 02:16:59|         264|         127|   NULL|                B00860|\n",
      "|              B01016|2019-10-02 05:47:05|2019-10-02 05:54:18|         264|          77|   NULL|                B01016|\n",
      "|              B03164|2019-10-03 07:26:48|2019-10-03 07:39:07|         264|          20|   NULL|                B03164|\n",
      "|              B01984|2019-10-02 10:33:00|2019-10-02 11:05:00|         264|          36|   NULL|                B01984|\n",
      "|              B00906|2019-10-01 05:08:55|2019-10-01 05:16:29|           7|           7|   NULL|                B00906|\n",
      "|              B03016|2019-10-02 11:09:03|2019-10-02 11:27:09|         264|         174|   NULL|                B02867|\n",
      "|              B02472|2019-10-02 09:14:43|2019-10-02 09:40:29|         264|          76|   NULL|                B02472|\n",
      "|              B01553|2019-10-02 17:14:22|2019-10-02 17:17:28|         264|          63|   NULL|                B01553|\n",
      "|              B01717|2019-10-02 14:49:03|2019-10-02 15:03:15|         264|         247|   NULL|                B01717|\n",
      "|              B03017|2019-10-01 09:43:53|2019-10-01 10:24:03|         226|         148|   NULL|                B03017|\n",
      "|              B01717|2019-10-02 10:01:45|2019-10-02 10:05:14|         264|         250|   NULL|                B01717|\n",
      "|              B00647|2019-10-01 09:13:37|2019-10-01 09:35:56|         264|         159|   NULL|                B00647|\n",
      "|              B01485|2019-10-02 04:43:52|2019-10-02 04:48:21|         264|          35|   NULL|                B02869|\n",
      "|              B02687|2019-10-01 12:50:38|2019-10-01 13:08:22|         210|         123|   NULL|                B02687|\n",
      "|              B02311|2019-10-02 16:18:23|2019-10-02 16:18:27|         264|         188|   NULL|                B02311|\n",
      "|              B03048|2019-10-02 08:27:12|2019-10-02 08:42:47|         264|         264|   NULL|                B03048|\n",
      "|              B00271|2019-10-01 05:43:08|2019-10-01 05:52:13|         264|         141|   NULL|                B00271|\n",
      "|              B03016|2019-10-01 08:51:36|2019-10-01 08:59:38|         264|         259|   NULL|                B02876|\n",
      "|              B01485|2019-10-03 10:39:08|2019-10-03 10:47:35|         264|          76|   NULL|                B01485|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|number_trips|\n",
      "+------------+\n",
      "|       62610|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(1) as number_trips\n",
    "    FROM fhv_trip_data\n",
    "    WHERE date_trunc('day', pickup_datetime) = '2019-10-15'\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|max(timestampdiff(HOUR, pickup_datetime, dropOff_datetime))|\n",
      "+-----------------------------------------------------------+\n",
      "|                                                     631152|\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT MAX(\n",
    "                TIMESTAMPDIFF(\n",
    "                    HOUR,\n",
    "                    pickup_datetime,\n",
    "                    dropOff_datetime\n",
    "                    )\n",
    "                )\n",
    "    FROM fhv_trip_data\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df.join(df_zones, df.PUlocationID == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+--------+--------------+------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|LocationID| Borough|          Zone|service_zone|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+--------+--------------+------------+\n",
      "|              B01537|2019-10-02 07:09:00|2019-10-02 07:23:00|         264|         264|   NULL|                B01537|       264| Unknown|            NV|         N/A|\n",
      "|              B00860|2019-10-02 01:58:19|2019-10-02 02:16:59|         264|         127|   NULL|                B00860|       264| Unknown|            NV|         N/A|\n",
      "|              B01016|2019-10-02 05:47:05|2019-10-02 05:54:18|         264|          77|   NULL|                B01016|       264| Unknown|            NV|         N/A|\n",
      "|              B03164|2019-10-03 07:26:48|2019-10-03 07:39:07|         264|          20|   NULL|                B03164|       264| Unknown|            NV|         N/A|\n",
      "|              B01984|2019-10-02 10:33:00|2019-10-02 11:05:00|         264|          36|   NULL|                B01984|       264| Unknown|            NV|         N/A|\n",
      "|              B00906|2019-10-01 05:08:55|2019-10-01 05:16:29|           7|           7|   NULL|                B00906|         7|  Queens|       Astoria|   Boro Zone|\n",
      "|              B03016|2019-10-02 11:09:03|2019-10-02 11:27:09|         264|         174|   NULL|                B02867|       264| Unknown|            NV|         N/A|\n",
      "|              B02472|2019-10-02 09:14:43|2019-10-02 09:40:29|         264|          76|   NULL|                B02472|       264| Unknown|            NV|         N/A|\n",
      "|              B01553|2019-10-02 17:14:22|2019-10-02 17:17:28|         264|          63|   NULL|                B01553|       264| Unknown|            NV|         N/A|\n",
      "|              B01717|2019-10-02 14:49:03|2019-10-02 15:03:15|         264|         247|   NULL|                B01717|       264| Unknown|            NV|         N/A|\n",
      "|              B03017|2019-10-01 09:43:53|2019-10-01 10:24:03|         226|         148|   NULL|                B03017|       226|  Queens|     Sunnyside|   Boro Zone|\n",
      "|              B01717|2019-10-02 10:01:45|2019-10-02 10:05:14|         264|         250|   NULL|                B01717|       264| Unknown|            NV|         N/A|\n",
      "|              B00647|2019-10-01 09:13:37|2019-10-01 09:35:56|         264|         159|   NULL|                B00647|       264| Unknown|            NV|         N/A|\n",
      "|              B01485|2019-10-02 04:43:52|2019-10-02 04:48:21|         264|          35|   NULL|                B02869|       264| Unknown|            NV|         N/A|\n",
      "|              B02687|2019-10-01 12:50:38|2019-10-01 13:08:22|         210|         123|   NULL|                B02687|       210|Brooklyn|Sheepshead Bay|   Boro Zone|\n",
      "|              B02311|2019-10-02 16:18:23|2019-10-02 16:18:27|         264|         188|   NULL|                B02311|       264| Unknown|            NV|         N/A|\n",
      "|              B03048|2019-10-02 08:27:12|2019-10-02 08:42:47|         264|         264|   NULL|                B03048|       264| Unknown|            NV|         N/A|\n",
      "|              B00271|2019-10-01 05:43:08|2019-10-01 05:52:13|         264|         141|   NULL|                B00271|       264| Unknown|            NV|         N/A|\n",
      "|              B03016|2019-10-01 08:51:36|2019-10-01 08:59:38|         264|         259|   NULL|                B02876|       264| Unknown|            NV|         N/A|\n",
      "|              B01485|2019-10-03 10:39:08|2019-10-03 10:47:35|         264|          76|   NULL|                B01485|       264| Unknown|            NV|         N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+--------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.drop('LocationID').createOrReplaceTempView('fhv_zone_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|       Zone|num_trips|\n",
      "+-----------+---------+\n",
      "|Jamaica Bay|        1|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          WITH trips_by_zone AS\n",
    "          (SELECT Zone, COUNT(*) AS num_trips\n",
    "          FROM fhv_zone_data\n",
    "          GROUP BY Zone)\n",
    "          SELECT Zone, num_trips\n",
    "          FROM trips_by_zone\n",
    "          ORDER BY num_trips ASC\n",
    "          LIMIT 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
